---
title: "Taste Analysis - multiple methods"
author: "Andrea Sonnellini"
date: "12 settembre 2020"
output: github_document
---

# Scope

In this script I will investigate data2.csv and build several models that describe the target variable Grade in terms of the given explanatory variables.
Model performances will be assessed via test errors.

# Preprocessing

```{r}

taste = read.table("D:/BIG_DATA/DSTI/OneDrive - Data ScienceTech Institute/2020-06-Advanced_Statistical_Analysis_ML/assignment/data2.csv", sep = ";", dec = ".", header = TRUE)

taste = taste[,-1] # remove product

head(taste)
dim(taste)
```

This dataset has only 16 individuals.

# Split training-test

Split dataset in train (75%) and test (25%).

```{r}

set.seed(1986)
#number of rows for the train set
row.train.num = floor(dim(taste)[1] * 0.75)
#row.test.num = dim(taste)[1] - row.train.num

#select the specific rows for train
row.train = sample(1:dim(taste)[1], row.train.num, replace = FALSE)

#train set
X.train = taste[row.train, 1:4]
Y.train = taste[row.train, 5]

#test set
X.test = taste[-row.train, 1:4]
Y.test = taste[-row.train, 5]
```

# Training set data exploration

I will check first the peculiarities of the explanatory variables in the training set.
```{r}
cor(X.train)
```

We can see a strong correlation betwee Bitter and Acid and a strong anti-correlation between Sugar and Acid and Sugar and Bitter.
So it could be interesting to do some variable selection in our models.

In what follows we will build linear regression models, CART and a Random Forest and select the best one based on their test errors.


# Linear model

I will check first the peculiarities of the full linear model with respect to the training set.

Adjusted R^2 is very low.
Residuals do not look centered, their symmetry is not exact.
```{r}
L = lm(Y.train ~ . , data = X.train)

summary(L)
```

Check assumptions on residuals: it is quite difficult to draw any conclusion with so few points.

Residuals do not seem centered and show a quadratic trend.

Residuals do not seem to be homoschedastic.

3 points are marked as potential outliers.

```{r}
plot(L)
```

Shapiro test for normality of the residuals - The shapiro test returns a p-value such that we do not reject H0, i.e. we cannot reject the fact that residuals are normal.

```{r}

shapiro.test(L$residuals)

```

Overall the "classic" assumptions on noise do not seem to be fulfilled, so we cannot use methods which rely on tests of significance to infer additional insights or perform variable selection.


## Variable selection in linear model 

In the following we will perform variable selection based on test error.
Given the quadratic-like trend in the residuals, I will include quadratic terms in the initial model.

```{r}
#include columns for the square of the expl variables
X.train.sq = data.frame(X.train, pul.sq = X.train[,"Pulpy"]^2, Sug.sq = X.train[,"Sugar"]^2, Aci.sq = X.train[,"Acid"]^2, Bit.sq = X.train[,"Bitter"]^2)

X.test.sq = data.frame(X.test, pul.sq = X.test[,"Pulpy"]^2, Sug.sq = X.test[,"Sugar"]^2, Aci.sq = X.test[,"Acid"]^2, Bit.sq = X.test[,"Bitter"]^2)

```

```{r}

num.comb = rep(NA,8) # store number of combinations for each value of i

for(i in 1:8){
  
  num.comb[i] = dim(as.matrix(combn(1:8, i)))[2] #number of combinations for a value of i

}

X.full.train = data.frame(X.train.sq,Y.train)

# matrix to save all possible test errors adjusted
store.results = matrix(NA,max(num.comb),8)

test = list() # list to save all possible combinations of i expl variables; i = 1,..,8.


```


```{r}

for(i in 1:8){
  #i drives the number of expl variable

  test[[i]] = as.matrix(combn(1:8, i)) # combn returns a matrix where each columns is a possible combination of i expl variables from an initial set of values from 1 to 8
  
  #this for iterates over the columns of test[[i]] which is a matrix as per the previous command
  for(j in 1:dim(test[[i]])[2] ){
    
    df.loop = X.full.train[,c(9,test[[i]][,j])] # dataset considering the j-th combination of i expl variables
    L.loop = lm(Y.train ~ ., data = df.loop) # corresponding model
    
    Y.hat.test = predict(L.loop, newdata = X.test.sq) # prediction on test set
    
    store.results[j,i] =  sqrt(sum((Y.test - Y.hat.test)^2)/ length(Y.test) )# test error
  }
  
}

xval = rep(1:8,each = max(num.comb))

```
The plot below shows the various test errors for each model

```{r}
plot(xval, as.vector(store.results), xlab = 'number of explanatory variables', ylab = 'test error')
```


It turns out that the linear model with "Acid", "Pulpy", "Pulpy.square" is the one having the smallest error, i.e. 0.812.

```{r}
#position of best test error in store.results 
ind.min = which(store.results == store.results[which.min(store.results)], arr.ind=TRUE)

store.results[ind.min] # best test error
Lfinal.test.err = store.results[ind.min]


best.param = test[[ ind.min[2] ]][,ind.min[1]] #expl var with best r2 adj

best.param

colnames(X.train.sq[,best.param]) # it turns out that the linear model with "Acid"   "Pulpy"  "pul.sq" is the one having the smallest error


```

Details of the selected model:

```{r}

Lfinal = lm(Y.train ~ . , data = X.full.train[, c(9, best.param) ]) #selected model

summary(Lfinal) # check all details of selected mode

```

# CART

## Build Max Tree
```{r}
library(rpart)

Tmax = rpart(Y.train~., data = X.train, control = rpart.control(minsplit = 2, cp = 10^(-9) ))

plot(Tmax)
text(Tmax)

#check it is max tree -OK
sum(Y.train != predict(Tmax, newdata = X.train))

```

```{r}
par.Tmax = printcp(Tmax) 
printcp(Tmax) 
plotcp(Tmax)
```


## Prune the tree

We obtain a tree with only the root node.
```{r}
pos.min.xerr = which.min(par.Tmax[,'xerror']) # row number of the tree with minimum xerror

threshold.tree = unique(par.Tmax[pos.min.xerr,"xerror"] + par.Tmax[pos.min.xerr,"xstd"]) #1SE threshold

row.prune = which(par.Tmax[,"xerror"] == par.Tmax[par.Tmax[,"xerror"]< threshold.tree,][,"xerror"][1])

alpha.opt = par.Tmax[row.prune,"CP"]

Tpruned = prune(Tmax, cp = alpha.opt)

plot(Tpruned)
text(Tpruned)
```

## Test Error for CART

```{r}
Y.hat.Cart.test = predict(Tpruned, newdata = X.test)

CART.test.error = sqrt(sum( (Y.test - Y.hat.Cart.test )^2 )/length(Y.test))

CART.test.error
```


# Random Forest

```{r}
library(randomForest)

forest = randomForest(Y.train~., data = X.train, ntree = 300)

forest
```

```{r}
Y.hat.Forest.test = predict(forest, newdata = X.test)

forest.test.error = sqrt(sum( (Y.test - Y.hat.Forest.test )^2 )/length(Y.test))

forest.test.error
```


# Conclusions

From the above procedure we obtained the following test errors for each model:


- Linear model with with "Acid" "Pulpy"  "pul.sq" ==> 0.8118648
- CART ==> 1.212699
- Random Forest ==> 1.14105

Based on the above, the linear model with "Acid" "Pulpy"  "pul.sq" is the model having the lowest test error.

Therefore the linear model has to be considered the best model among the proposed ones.
The final model trained on the full dataset is then:

```{r}
enhanced_taste = data.frame(taste, pul.sq = taste[,"Pulpy"]^2)

Lfinal.full = lm(Grade ~ Acid + Pulpy + pul.sq, data = enhanced_taste)

Lfinal.full

```


$$ Grade = -5.5489 -0.4398 * Acid + 13.0579 * Pulpy -3.0718 * Pulpy^2 $$


On the other hand we have to stress that with so few points in the dataset, we expect the obtained results (in particular the explanatory variables selected for the linear model) to be quite unstable with respect to changes in the initial splitting of the dataset in training and test set.
In other words, rerunning the above script with a different split training-set will most likely lead to different results for what concerns: 

- the variables selected in the linear model
- the CART tree nodes/splits

# Possible Improvements

A possible solution to mitigate the instability related to the initial splitting in training and test set could be to perform model selection via cross vfold validation. Given the small amount of individuals probably it could be worth to consider bootstrapped samples.

I did not implement this alternative solution due to the lack of time.